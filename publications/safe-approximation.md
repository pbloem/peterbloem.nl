---
title: A safe approximation of Kolmogorov complexity
parent: publications

---

<header>
<h1>A safe approximation of Kolmogorov complexity</h1>

</header>
<ul class="links">
<li>
<a href="/files/a-safe-approximation-of-kolmogorov-complexity.pdf">download the article</a>
</li>
<li>
<a href="/files/a-safe-approximation-of-kolmogorov-complexity.presentation.pdf">download the presentation</a>
</li>
</ul>

<figure>
<iframe width="660" height="315" src="https://www.youtube.com/embed/-rifN-b9ovo?si=Jp3YKUoN6ViaD4xf" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<figcaption>
A presentation I gave in 2022 at <a href="https://sites.google.com/site/boumedienehamzi/symposium-on-algorithmic-information-theory-and-machine-learning?authuser=0">the AIT & ML symposium</a>. <a href="/files/KCLondon.2022.pdf">Slides here</a>.
</figcaption>
</figure>

Kolmogorov complexity is an incomputable function. It can be approximated from above but not to arbitrary given precision and it cannot be approximated from below. By restricting the source of the data to a specific model class, we can construct a computable function to approximate the Kolmogorov complexity in a probabilistic sense: the probability that the error is greater than _k_ bits decays exponentially with _k_. We apply the same method to the normalized information distance and discuss conditions that affect the safety of the approximation.